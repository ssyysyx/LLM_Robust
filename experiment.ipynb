{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# 临时设置环境变量\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "\n",
    "# 测试请求\n",
    "response = requests.get('https://www.google.com')\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义openai的Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class OpenaiClient:\n",
    "    def __init__(self, keys=None, start_id=None, proxy=None):\n",
    "        import openai\n",
    "        from openai import OpenAI\n",
    "        \n",
    "        if isinstance(keys, str):\n",
    "            keys = [keys]\n",
    "        if keys is None:\n",
    "            raise \"Please provide OpenAI Key.\"\n",
    "\n",
    "        self.key = keys\n",
    "        self.key_id = start_id or 0\n",
    "        self.key_id = self.key_id % len(self.key)\n",
    "        self.api_key = self.key[self.key_id % len(self.key)]\n",
    "        # 下面这一行base_url=\"https://api.gpts.vin/v1\"是我自己加的\n",
    "        # self.client = OpenAI(base_url=\"https://uiuiapi.com/v1\", api_key=self.api_key)\n",
    "        self.client = OpenAI(api_key=self.api_key)\n",
    "\n",
    "    def chat(self, *args, return_text=False, reduce_length=False, **kwargs):\n",
    "        while True:\n",
    "            try:\n",
    "                completion = self.client.chat.completions.create(*args, **kwargs, timeout=30)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                if \"This model's maximum context length is\" in str(e):\n",
    "                    print('reduce_length')\n",
    "                    return 'ERROR::reduce_length'\n",
    "                time.sleep(0.1)\n",
    "        if return_text:\n",
    "            completion = completion.choices[0].message.content\n",
    "        return completion\n",
    "\n",
    "    def text(self, *args, return_text=False, reduce_length=False, **kwargs):\n",
    "        while True:\n",
    "            try:\n",
    "                completion = self.client.completions.create(\n",
    "                    *args, **kwargs\n",
    "                )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                if \"This model's maximum context length is\" in str(e):\n",
    "                    print('reduce_length')\n",
    "                    return 'ERROR::reduce_length'\n",
    "                time.sleep(0.1)\n",
    "        if return_text:\n",
    "            completion = completion.choices[0].text\n",
    "        return completion\n",
    "\n",
    "def run_llm(messages, api_key=None, model_name=\"gpt-3.5-turbo\"):\n",
    "    if 'gpt' in model_name:\n",
    "        Client = OpenaiClient\n",
    "    # elif 'o1' in model_name:\n",
    "    #     Client = OpenaiClient_o1\n",
    "    # elif 'claude' in model_name:\n",
    "    #     Client = ClaudeClient\n",
    "    # elif 'gemini' in model_name:\n",
    "    #     Client = GeminiClient\n",
    "    # elif 'moonshot' in model_name:\n",
    "    #     Client = KimiClient\n",
    "    # elif 'deepseek' in model_name:\n",
    "    #     Client = BailianClient\n",
    "    # else:\n",
    "    #     Client = LitellmClient\n",
    "\n",
    "    agent = Client(api_key)\n",
    "    response = agent.chat(model=model_name, messages=messages, temperature=0, return_text=True) #temperature used to be 0\n",
    "    # print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集 导入举例\n",
    "imdb:25000条\n",
    "yelp:38000条\n",
    "sst:2210条  \n",
    "label:0 negative label:1 positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = load_dataset(\"./stanfordnlp/imdb\")\n",
    "# dataset = load_dataset(\"./fancyzhx/yelp\")\n",
    "# dataset = load_dataset(\"./stanfordnlp/sst\")\n",
    "# test_dataset = dataset[\"test\"]\n",
    "# test_dataset[12500][\"text\"]\n",
    "# print(test_dataset[12500][\"label\"])\n",
    "\n",
    "# dataset = load_dataset(\"./stanfordnlp/imdb\", split=\"test[:100]\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义字符级攻击函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# 定义字符级攻击函数\n",
    "def text_attack(text, p=0.999, max_alterations=10000000000):\n",
    "    words = text.split()\n",
    "    alterations = 0\n",
    "    attacked_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        if alterations >= max_alterations:\n",
    "            attacked_words.append(word)\n",
    "            continue\n",
    "        \n",
    "        # Introduce randomness to alter characters within the word\n",
    "        if len(word) > 1 and random.random() < p:\n",
    "            word_list = list(word)  # Convert word to list for manipulation\n",
    "            \n",
    "            # Randomly delete a character from the word\n",
    "            if random.random() < 0.5 and len(word_list) > 1:\n",
    "                idx = random.randrange(len(word_list))\n",
    "                word_list.pop(idx)\n",
    "            \n",
    "            # Randomly change a character (substitution)\n",
    "            if random.random() < 0.5:\n",
    "                idx = random.randrange(len(word_list))\n",
    "                word_list[idx] = random.choice('abcdefghijklmnopqrstuvwxyz')  # Random char substitution\n",
    "            \n",
    "            # Randomly swap characters in the word\n",
    "            if random.random() < 0.5 and len(word_list) > 1:\n",
    "                idx1, idx2 = random.sample(range(len(word_list)), 2)  # Pick two random indices to swap\n",
    "                word_list[idx1], word_list[idx2] = word_list[idx2], word_list[idx1]\n",
    "            \n",
    "            # Rebuild the word after modification\n",
    "            new_word = ''.join(word_list)\n",
    "            attacked_words.append(new_word)\n",
    "            alterations += 1\n",
    "        else:\n",
    "            attacked_words.append(word)\n",
    "    \n",
    "    return \" \".join(attacked_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_messages(text):\n",
    "    messages = [\n",
    "        {'role': 'system',\n",
    "         'content': \"You are a helpful assistant specialized in sentiment analysis.\"},\n",
    "        {'role': 'user',\n",
    "         'content': f\"Please classify the sentiment of the following review:\\n\\n\\\"{text}\\\"\\nRespond **only** with 'positive' or 'negative'. Do not include any extra information.\"}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import openai\n",
    "openai_key = 'sk-'\n",
    "model_names = ['gpt-3.5-turbo', 'gpt-4-turbo','gpt-4','gpt-4o']\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 加载数据集\n",
    "dataset_name = \"./stanfordnlp/imdb\"\n",
    "# dataset_name = \"./fancyzhx/yelp\"\n",
    "dataset = load_dataset(dataset_name, split=\"test\")\n",
    "dataset = list(dataset)\n",
    "dataset = random.sample(dataset, 500)\n",
    "\n",
    "for model_name in model_names:\n",
    "    # 用来保存结果的列表\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(len(dataset)), desc=\"Processing\", unit=\"item\"):\n",
    "        item = dataset[i]\n",
    "        original_text = item[\"text\"]\n",
    "        # print(\"原始文本：\", original_text)\n",
    "\n",
    "        # 攻击文本\n",
    "        attacked_text = text_attack(original_text)\n",
    "        # print(\"\\n攻击后文本：\", attacked_text)\n",
    "\n",
    "        messages_attacked = create_messages(attacked_text)\n",
    "        messages_original = create_messages(original_text)\n",
    "\n",
    "        # 获取原始文本和攻击后文本的情感分析结果\n",
    "        result_original = run_llm(messages_original, api_key=openai_key, model_name=model_name)\n",
    "        result_attacked = run_llm(messages_attacked, api_key=openai_key, model_name=model_name)\n",
    "\n",
    "        # 将结果保存到列表中\n",
    "        results.append({\n",
    "            \"id\": i,\n",
    "            \"label\": item[\"label\"],\n",
    "            \"result_original\": result_original,\n",
    "            \"result_attacked\": result_attacked,\n",
    "            \"original_text\": original_text,\n",
    "            \"attacked_text\": attacked_text,\n",
    "        })\n",
    "\n",
    "    # 保存为 CSV 和 PKL 文件\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(f'{dataset_name}_{model_name}_results.csv', index=False)  # 保存为 CSV\n",
    "\n",
    "    with open(f'{dataset_name}_{model_name}_results.pkl', 'wb') as f:\n",
    "        pickle.dump\n",
    "\n",
    "    df.to_pickle(f'{dataset_name}_{model_name}_results.pkl')  # 保存为 PKL\n",
    "\n",
    "    print(\"保存完毕。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import openai\n",
    "openai_key = 'sk-'\n",
    "model_names = ['gpt-3.5-turbo', 'gpt-4-turbo','gpt-4','gpt-4o']\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 加载数据集\n",
    "# dataset_name = \"./stanfordnlp/imdb\"\n",
    "dataset_name = \"./fancyzhx/yelp\"\n",
    "dataset = load_dataset(dataset_name, split=\"test\")\n",
    "dataset = list(dataset)\n",
    "dataset = random.sample(dataset, 500)\n",
    "\n",
    "for model_name in model_names:\n",
    "    # 用来保存结果的列表\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(len(dataset)), desc=\"Processing\", unit=\"item\"):\n",
    "        item = dataset[i]\n",
    "        original_text = item[\"text\"]\n",
    "        # print(\"原始文本：\", original_text)\n",
    "\n",
    "        # 攻击文本\n",
    "        attacked_text = text_attack(original_text)\n",
    "        # print(\"\\n攻击后文本：\", attacked_text)\n",
    "\n",
    "        messages_attacked = create_messages(attacked_text)\n",
    "        messages_original = create_messages(original_text)\n",
    "\n",
    "        # 获取原始文本和攻击后文本的情感分析结果\n",
    "        result_original = run_llm(messages_original, api_key=openai_key, model_name=model_name)\n",
    "        result_attacked = run_llm(messages_attacked, api_key=openai_key, model_name=model_name)\n",
    "\n",
    "        # 将结果保存到列表中\n",
    "        results.append({\n",
    "            \"id\": i,\n",
    "            \"label\": item[\"label\"],\n",
    "            \"result_original\": result_original,\n",
    "            \"result_attacked\": result_attacked,\n",
    "            \"original_text\": original_text,\n",
    "            \"attacked_text\": attacked_text,\n",
    "        })\n",
    "\n",
    "    # 保存为 CSV 和 PKL 文件\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(f'{dataset_name}_{model_name}_results.csv', index=False)  # 保存为 CSV\n",
    "    df.to_pickle(f'{dataset_name}_{model_name}_results.pkl')  # 保存为 PKL\n",
    "\n",
    "    print(\"保存完毕。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "直接上sentiment-analysis的过程（不用大模型）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "\n",
    "# # 初始化一个情感分析模型\n",
    "# # Specify the local model path\n",
    "# model_path = \"./model/sentiment-analysis\"\n",
    "\n",
    "# # Initialize the sentiment analysis model with the local path\n",
    "# classifier = pipeline(\"sentiment-analysis\", model=model_path)\n",
    "\n",
    "# # 测试其中一条评论\n",
    "# sample_text = dataset[0][\"text\"]\n",
    "# print(\"原始文本：\", sample_text)\n",
    "\n",
    "# attacked_text = text_attack(sample_text)\n",
    "# print(\"\\n攻击后文本：\", attacked_text)\n",
    "\n",
    "# # 分别做推理\n",
    "# result_original = classifier(sample_text)\n",
    "# result_attacked = classifier(attacked_text)\n",
    " \n",
    "# print(\"\\n原始文本的情感分析结果：\", result_original)\n",
    "# print(\"攻击后文本的情感分析结果：\", result_attacked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [32:05<00:00,  3.85s/item]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存完毕。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [33:44<00:00,  4.05s/item]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存完毕。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|██████████| 500/500 [31:21<00:00,  3.76s/item]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存完毕。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import openai\n",
    "openai_key = 'sk-'\n",
    "model_names = ['gpt-3.5-turbo', 'gpt-4-turbo','gpt-4o']\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 加载数据集\n",
    "# dataset_name = \"./stanfordnlp/imdb\"\n",
    "# dataset_name = \"./fancyzhx/yelp\"\n",
    "dataset_name = \"stanfordnlp/sst-2\"\n",
    "dataset = load_dataset(dataset_name, split=\"validation\")\n",
    "dataset = list(dataset)\n",
    "dataset = random.sample(dataset, 500)\n",
    "\n",
    "for model_name in model_names:\n",
    "    # 用来保存结果的列表\n",
    "    results = []\n",
    "\n",
    "    for i in tqdm(range(len(dataset)), desc=\"Processing\", unit=\"item\"):\n",
    "        item = dataset[i]\n",
    "        original_text = item[\"sentence\"]\n",
    "        # print(\"原始文本：\", original_text)\n",
    "\n",
    "        # 攻击文本\n",
    "        attacked_text = text_attack(original_text)\n",
    "        # print(\"\\n攻击后文本：\", attacked_text)\n",
    "\n",
    "        messages_attacked = create_messages(attacked_text)\n",
    "        messages_original = create_messages(original_text)\n",
    "\n",
    "        # 获取原始文本和攻击后文本的情感分析结果\n",
    "        result_original = run_llm(messages_original, api_key=openai_key, model_name=model_name)\n",
    "        result_attacked = run_llm(messages_attacked, api_key=openai_key, model_name=model_name)\n",
    "\n",
    "        # 将结果保存到列表中\n",
    "        results.append({\n",
    "            \"id\": i,\n",
    "            \"label\": item[\"label\"],\n",
    "            \"result_original\": result_original,\n",
    "            \"result_attacked\": result_attacked,\n",
    "            \"original_text\": original_text,\n",
    "            \"attacked_text\": attacked_text,\n",
    "        })\n",
    "\n",
    "    # 保存为 CSV 和 PKL 文件\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_csv(f'{dataset_name}_{model_name}_results.csv', index=False)  # 保存为 CSV\n",
    "    df.to_pickle(f'{dataset_name}_{model_name}_results.pkl')  # 保存为 PKL\n",
    "\n",
    "    print(\"保存完毕。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"stanfordnlp/sst-2\"\n",
    "dataset = load_dataset(dataset_name, split=\"validation\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
