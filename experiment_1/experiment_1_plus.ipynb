{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# 临时设置环境变量\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "\n",
    "# 测试请求\n",
    "response = requests.get('https://www.google.com')\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义openai的Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class OpenaiClient:\n",
    "    def __init__(self, keys=None, start_id=None, proxy=None):\n",
    "        import openai\n",
    "        from openai import OpenAI\n",
    "        \n",
    "        if isinstance(keys, str):\n",
    "            keys = [keys]\n",
    "        if keys is None:\n",
    "            raise \"Please provide OpenAI Key.\"\n",
    "\n",
    "        self.key = keys\n",
    "        self.key_id = start_id or 0\n",
    "        self.key_id = self.key_id % len(self.key)\n",
    "        self.api_key = self.key[self.key_id % len(self.key)]\n",
    "        # 下面这一行base_url=\"https://api.gpts.vin/v1\"是我自己加的\n",
    "        # self.client = OpenAI(base_url=\"https://uiuiapi.com/v1\", api_key=self.api_key)\n",
    "        self.client = OpenAI(api_key=self.api_key)\n",
    "\n",
    "    def chat(self, *args, return_text=False, reduce_length=False, **kwargs):\n",
    "        while True:\n",
    "            try:\n",
    "                completion = self.client.chat.completions.create(*args, **kwargs, timeout=30)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                if \"This model's maximum context length is\" in str(e):\n",
    "                    print('reduce_length')\n",
    "                    return 'ERROR::reduce_length'\n",
    "                time.sleep(0.1)\n",
    "        if return_text:\n",
    "            completion = completion.choices[0].message.content\n",
    "        return completion\n",
    "\n",
    "    def text(self, *args, return_text=False, reduce_length=False, **kwargs):\n",
    "        while True:\n",
    "            try:\n",
    "                completion = self.client.completions.create(\n",
    "                    *args, **kwargs\n",
    "                )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                if \"This model's maximum context length is\" in str(e):\n",
    "                    print('reduce_length')\n",
    "                    return 'ERROR::reduce_length'\n",
    "                time.sleep(0.1)\n",
    "        if return_text:\n",
    "            completion = completion.choices[0].text\n",
    "        return completion\n",
    "\n",
    "def run_llm(messages, api_key=None, model_name=\"gpt-3.5-turbo\"):\n",
    "    if 'gpt' in model_name:\n",
    "        Client = OpenaiClient\n",
    "    # elif 'o1' in model_name:\n",
    "    #     Client = OpenaiClient_o1\n",
    "    # elif 'claude' in model_name:\n",
    "    #     Client = ClaudeClient\n",
    "    # elif 'gemini' in model_name:\n",
    "    #     Client = GeminiClient\n",
    "    # elif 'moonshot' in model_name:\n",
    "    #     Client = KimiClient\n",
    "    # elif 'deepseek' in model_name:\n",
    "    #     Client = BailianClient\n",
    "    # else:\n",
    "    #     Client = LitellmClient\n",
    "\n",
    "    agent = Client(api_key)\n",
    "    response = agent.chat(model=model_name, messages=messages, temperature=0, return_text=True) #temperature used to be 0\n",
    "    # print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集 导入举例\n",
    "imdb:25000条\n",
    "yelp:38000条\n",
    "sst:2210条  \n",
    "label:0 negative label:1 positive\n",
    "\n",
    "数据集报错可能是缺文件fancyzhx/yelp/plain_text的train\n",
    "stanfordnlp/imdb/train和unsupervised\n",
    "stanfordnlp/sst-2/data/test和train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = load_dataset(\"./stanfordnlp/imdb\")\n",
    "# dataset = load_dataset(\"./fancyzhx/yelp\")\n",
    "# dataset = load_dataset(\"./stanfordnlp/sst\")\n",
    "# test_dataset = dataset[\"test\"]\n",
    "# test_dataset[12500][\"text\"]\n",
    "# print(test_dataset[12500][\"label\"])\n",
    "\n",
    "# dataset = load_dataset(\"./stanfordnlp/imdb\", split=\"test[:100]\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义字符级攻击函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def text_attack(text, \n",
    "                                          perturb_prob=1.0, \n",
    "                                          max_alterations=10, \n",
    "                                          max_per_word=3, \n",
    "                                          p_delete=0.33, \n",
    "                                          p_replace=0.33, \n",
    "                                          p_insert=0.34):\n",
    "    \"\"\"\n",
    "    执行字符级扰动并返回统计信息（安全版本，避免死循环）\n",
    "    - text：输入文本\n",
    "    - perturb_prob：每个词被扰动的概率\n",
    "    - max_alterations：全局最大扰动次数\n",
    "    - max_per_word：每个词最多扰动次数\n",
    "    - p_delete / p_replace / p_insert：三类扰动的概率分布（总和应为1.0）\n",
    "    \n",
    "    返回：\n",
    "    - 扰动后的文本\n",
    "    - 字典形式的扰动统计信息\n",
    "    \"\"\"\n",
    "    assert abs(p_delete + p_replace + p_insert - 1.0) < 1e-5, \"扰动概率之和必须为 1.0\"\n",
    "\n",
    "    words = text.split()\n",
    "    attacked_words = []\n",
    "    alterations = 0\n",
    "\n",
    "    # 扰动类型统计\n",
    "    count_delete, count_replace, count_insert = 0, 0, 0\n",
    "\n",
    "    for word in words:\n",
    "        if alterations >= max_alterations or random.random() > perturb_prob:\n",
    "            attacked_words.append(word)\n",
    "            continue\n",
    "\n",
    "        # 特殊处理：只允许删除时避免词长为1死循环\n",
    "        if len(word) <= 1 and p_delete == 1.0:\n",
    "            attacked_words.append(word)\n",
    "            continue\n",
    "\n",
    "        word_list = list(word)\n",
    "        local_alter = 0\n",
    "        loop_counter = 0\n",
    "        max_loops = 20  # 防止个别词进入死循环\n",
    "\n",
    "        while alterations < max_alterations and local_alter < max_per_word and len(word_list) > 0:\n",
    "            loop_counter += 1\n",
    "            if loop_counter > max_loops:\n",
    "                print(f\"⚠️ 跳出 `{word}` 的扰动循环，避免死循环\")\n",
    "                break\n",
    "\n",
    "            perturb_type = random.choices(\n",
    "                population=[\"delete\", \"replace\", \"insert\"],\n",
    "                weights=[p_delete, p_replace, p_insert],\n",
    "                k=1\n",
    "            )[0]\n",
    "\n",
    "            if perturb_type == \"delete\" and len(word_list) > 1:\n",
    "                idx = random.randrange(len(word_list))\n",
    "                word_list.pop(idx)\n",
    "                alterations += 1\n",
    "                local_alter += 1\n",
    "                count_delete += 1\n",
    "\n",
    "            elif perturb_type == \"replace\" and len(word_list) > 0:\n",
    "                idx = random.randrange(len(word_list))\n",
    "                word_list[idx] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "                alterations += 1\n",
    "                local_alter += 1\n",
    "                count_replace += 1\n",
    "\n",
    "            elif perturb_type == \"insert\":\n",
    "                idx = random.randint(0, len(word_list))\n",
    "                word_list.insert(idx, random.choice('abcdefghijklmnopqrstuvwxyz'))\n",
    "                alterations += 1\n",
    "                local_alter += 1\n",
    "                count_insert += 1\n",
    "\n",
    "        attacked_words.append(\"\".join(word_list))\n",
    "\n",
    "    stats = {\n",
    "        \"total_alterations\": alterations,\n",
    "        \"delete\": count_delete,\n",
    "        \"replace\": count_replace,\n",
    "        \"insert\": count_insert\n",
    "    }\n",
    "\n",
    "    return \" \".join(attacked_words), stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_messages(text):\n",
    "    messages = [\n",
    "        {'role': 'system',\n",
    "         'content': \"You are a helpful assistant specialized in sentiment analysis.\"},\n",
    "        {'role': 'user',\n",
    "         'content': f\"Please classify the sentiment of the following review:\\n\\n\\\"{text}\\\"\\nRespond **only** with 'positive' or 'negative'. Do not include any extra information.\"}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# 加载数据集\n",
    "# dataset_name = \"./stanfordnlp/imdb\"\n",
    "# dataset_name = \"./fancyzhx/yelp\"\n",
    "dataset_name = \"./stanfordnlp/sst-2\"\n",
    "dataset = load_dataset(dataset_name, split=\"validation\")\n",
    "dataset = list(dataset)\n",
    "dataset = random.sample(dataset, 278)\n",
    "\n",
    "with open(\"./dataset_278/sst-2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据集、且预生成扰动文本，缓存下来:imdb, yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_settings = [\n",
    "    {\"name\": \"delete\", \"p_delete\": 1.0, \"p_replace\": 0.0, \"p_insert\": 0.0},\n",
    "    {\"name\": \"replace\", \"p_delete\": 0.0, \"p_replace\": 1.0, \"p_insert\": 0.0},\n",
    "    {\"name\": \"insert\", \"p_delete\": 0.0, \"p_replace\": 0.0, \"p_insert\": 1.0},\n",
    "    {\"name\": \"mix\", \"p_delete\": 0.33, \"p_replace\": 0.34, \"p_insert\": 0.33},\n",
    "]\n",
    "\n",
    "# 设置随机种子，保证攻击文本一致\n",
    "random.seed(42)\n",
    "\n",
    "# 加载数据集\n",
    "datasets = [\"imdb\", \"yelp\"]\n",
    "# 从 pickle 文件中加载回来\n",
    "for dataset_name in datasets:\n",
    "    with open(f\"./dataset_278/{dataset_name}.pkl\", \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "        # dataset = dataset[0:1]\n",
    "\n",
    "    for attack_config in attack_settings:\n",
    "        setting_name = attack_config[\"name\"]\n",
    "        p_delete = attack_config[\"p_delete\"]\n",
    "        p_replace = attack_config[\"p_replace\"]\n",
    "        p_insert = attack_config[\"p_insert\"]\n",
    "\n",
    "        # Step 1: 预生成扰动文本，缓存下来\n",
    "        attacked_samples = []\n",
    "        for item in dataset:\n",
    "            original_text = item[\"text\"]\n",
    "            attacked_text = text_attack(\n",
    "                original_text,\n",
    "                perturb_prob=0.95,\n",
    "                max_alterations=1000,\n",
    "                max_per_word=2,\n",
    "                p_delete=p_delete,\n",
    "                p_replace=p_replace,\n",
    "                p_insert=p_insert\n",
    "            )\n",
    "            attacked_samples.append({\n",
    "                \"label\": item[\"label\"],\n",
    "                \"original_text\": original_text,\n",
    "                \"attacked_text\": attacked_text\n",
    "            })\n",
    "\n",
    "        print(attacked_samples)\n",
    "        print(len(attacked_samples))\n",
    "        # 保存扰动样本为 pickle 文件\n",
    "        save_path = f'./text_processed_278/{dataset_name}_{setting_name}.pkl'\n",
    "\n",
    "        # 确保目标文件夹存在\n",
    "        import os\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(attacked_samples, f)\n",
    "\n",
    "        import pandas as pd\n",
    "        # 保存为 CSV 文件\n",
    "        df = pd.DataFrame(attacked_samples)\n",
    "        df.to_csv(f'./text_processed_278/{dataset_name}_{setting_name}.csv', index=False)  # 保存为 CSV\n",
    "\n",
    "        print(f\"✅ 已保存：{save_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据集、且预生成扰动文本，缓存下来:sst-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "\n",
    "attack_settings = [\n",
    "    {\"name\": \"delete\", \"p_delete\": 1.0, \"p_replace\": 0.0, \"p_insert\": 0.0},\n",
    "    {\"name\": \"replace\", \"p_delete\": 0.0, \"p_replace\": 1.0, \"p_insert\": 0.0},\n",
    "    {\"name\": \"insert\", \"p_delete\": 0.0, \"p_replace\": 0.0, \"p_insert\": 1.0},\n",
    "    {\"name\": \"mix\", \"p_delete\": 0.33, \"p_replace\": 0.34, \"p_insert\": 0.33},\n",
    "]\n",
    "\n",
    "# 设置随机种子，保证攻击文本一致\n",
    "random.seed(42)\n",
    "\n",
    "# 加载数据集\n",
    "datasets = [\"sst-2\"]\n",
    "# 从 pickle 文件中加载回来\n",
    "for dataset_name in datasets:\n",
    "    with open(f\"./dataset_278/{dataset_name}.pkl\", \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "        # dataset = dataset[0:1]\n",
    "\n",
    "    for attack_config in attack_settings:\n",
    "        setting_name = attack_config[\"name\"]\n",
    "        p_delete = attack_config[\"p_delete\"]\n",
    "        p_replace = attack_config[\"p_replace\"]\n",
    "        p_insert = attack_config[\"p_insert\"]\n",
    "\n",
    "        # Step 1: 预生成扰动文本，缓存下来\n",
    "        attacked_samples = []\n",
    "        for item in dataset:\n",
    "            original_text = item[\"sentence\"]\n",
    "            attacked_text = text_attack(\n",
    "                original_text,\n",
    "                perturb_prob=0.95,\n",
    "                max_alterations=1000,\n",
    "                max_per_word=2,\n",
    "                p_delete=p_delete,\n",
    "                p_replace=p_replace,\n",
    "                p_insert=p_insert\n",
    "            )\n",
    "            attacked_samples.append({\n",
    "                \"label\": item[\"label\"],\n",
    "                \"original_text\": original_text,\n",
    "                \"attacked_text\": attacked_text\n",
    "            })\n",
    "\n",
    "        print(attacked_samples)\n",
    "        print(len(attacked_samples))\n",
    "        # 保存扰动样本为 pickle 文件\n",
    "        save_path = f'./text_processed_278/{dataset_name}_{setting_name}.pkl'\n",
    "\n",
    "        # 确保目标文件夹存在\n",
    "        import os\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(attacked_samples, f)\n",
    "\n",
    "        import pandas as pd\n",
    "        # 保存为 CSV 文件\n",
    "        df = pd.DataFrame(attacked_samples)\n",
    "        df.to_csv(f'./text_processed_278/{dataset_name}_{setting_name}.csv', index=False)  # 保存为 CSV\n",
    "\n",
    "        print(f\"✅ 已保存：{save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM 判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️ 已完成，跳过：imdb | gpt-3.5-turbo | delete\n",
      "⏭️ 已完成，跳过：imdb | gpt-4-turbo | delete\n",
      "⏭️ 已完成，跳过：imdb | gpt-4o | delete\n",
      "⏭️ 已完成，跳过：imdb | gpt-3.5-turbo | replace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | replace | gpt-4-turbo: 100%|██████████| 278/278 [21:45<00:00,  4.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4-turbo | replace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | replace | gpt-4o: 100%|██████████| 278/278 [19:02<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4o | replace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | insert | gpt-3.5-turbo: 100%|██████████| 278/278 [17:09<00:00,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-3.5-turbo | insert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | insert | gpt-4-turbo: 100%|██████████| 278/278 [21:59<00:00,  4.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4-turbo | insert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | insert | gpt-4o: 100%|██████████| 278/278 [18:25<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4o | insert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | mix | gpt-3.5-turbo: 100%|██████████| 278/278 [17:37<00:00,  3.80s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-3.5-turbo | mix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | mix | gpt-4-turbo: 100%|██████████| 278/278 [21:47<00:00,  4.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4-turbo | mix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | mix | gpt-4o: 100%|██████████| 278/278 [17:20<00:00,  3.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4o | mix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yelp | delete | gpt-3.5-turbo: 100%|██████████| 278/278 [16:38<00:00,  3.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: yelp | gpt-3.5-turbo | delete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yelp | delete | gpt-4-turbo: 100%|██████████| 278/278 [19:33<00:00,  4.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: yelp | gpt-4-turbo | delete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yelp | delete | gpt-4o: 100%|██████████| 278/278 [16:29<00:00,  3.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: yelp | gpt-4o | delete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yelp | replace | gpt-3.5-turbo: 100%|██████████| 278/278 [16:41<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: yelp | gpt-3.5-turbo | replace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yelp | replace | gpt-4-turbo: 100%|██████████| 278/278 [19:55<00:00,  4.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: yelp | gpt-4-turbo | replace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yelp | replace | gpt-4o: 100%|██████████| 278/278 [16:42<00:00,  3.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: yelp | gpt-4o | replace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yelp | insert | gpt-3.5-turbo: 100%|██████████| 278/278 [16:22<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: yelp | gpt-3.5-turbo | insert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yelp | insert | gpt-4-turbo: 100%|██████████| 278/278 [20:04<00:00,  4.33s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: yelp | gpt-4-turbo | insert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yelp | insert | gpt-4o: 100%|██████████| 278/278 [16:34<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: yelp | gpt-4o | insert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yelp | mix | gpt-3.5-turbo: 100%|██████████| 278/278 [15:47<00:00,  3.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: yelp | gpt-3.5-turbo | mix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yelp | mix | gpt-4-turbo: 100%|██████████| 278/278 [19:45<00:00,  4.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: yelp | gpt-4-turbo | mix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yelp | mix | gpt-4o: 100%|██████████| 278/278 [15:56<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: yelp | gpt-4o | mix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sst-2 | delete | gpt-3.5-turbo: 100%|██████████| 278/278 [15:55<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: sst-2 | gpt-3.5-turbo | delete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sst-2 | delete | gpt-4-turbo: 100%|██████████| 278/278 [18:10<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: sst-2 | gpt-4-turbo | delete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sst-2 | delete | gpt-4o: 100%|██████████| 278/278 [16:35<00:00,  3.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: sst-2 | gpt-4o | delete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sst-2 | replace | gpt-3.5-turbo: 100%|██████████| 278/278 [15:46<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: sst-2 | gpt-3.5-turbo | replace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sst-2 | replace | gpt-4-turbo: 100%|██████████| 278/278 [18:44<00:00,  4.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: sst-2 | gpt-4-turbo | replace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sst-2 | replace | gpt-4o: 100%|██████████| 278/278 [16:14<00:00,  3.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: sst-2 | gpt-4o | replace\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sst-2 | insert | gpt-3.5-turbo: 100%|██████████| 278/278 [15:44<00:00,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: sst-2 | gpt-3.5-turbo | insert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sst-2 | insert | gpt-4-turbo: 100%|██████████| 278/278 [18:54<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: sst-2 | gpt-4-turbo | insert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sst-2 | insert | gpt-4o: 100%|██████████| 278/278 [17:25<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: sst-2 | gpt-4o | insert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sst-2 | mix | gpt-3.5-turbo: 100%|██████████| 278/278 [16:20<00:00,  3.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: sst-2 | gpt-3.5-turbo | mix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sst-2 | mix | gpt-4-turbo: 100%|██████████| 278/278 [18:22<00:00,  3.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: sst-2 | gpt-4-turbo | mix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sst-2 | mix | gpt-4o: 100%|██████████| 278/278 [16:33<00:00,  3.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: sst-2 | gpt-4o | mix\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 已完成的组合\n",
    "finished = {\n",
    "    ('imdb', 'gpt-3.5-turbo', 'delete'),\n",
    "    ('imdb', 'gpt-3.5-turbo', 'replace'),\n",
    "    ('imdb', 'gpt-4-turbo', 'delete'),\n",
    "    ('imdb', 'gpt-4o', 'delete'),\n",
    "}\n",
    "\n",
    "import openai\n",
    "openai_key = 'sk-' # 补全\n",
    "model_names = ['gpt-3.5-turbo', 'gpt-4-turbo','gpt-4o']\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# 模型名 & 数据集 & 攻击模式\n",
    "model_names = ['gpt-3.5-turbo', 'gpt-4-turbo', 'gpt-4o']\n",
    "datasets = ['imdb', 'yelp', 'sst-2']\n",
    "attack_modes = ['delete', 'replace', 'insert', 'mix']\n",
    "\n",
    "# 输出路径\n",
    "output_dir = './result_278'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    for attack_mode in attack_modes:\n",
    "        # 加载攻击样本\n",
    "        file_path = f'./text_processed_278/{dataset_name}_{attack_mode}.pkl'\n",
    "        with open(file_path, 'rb') as f:\n",
    "            attacked_samples = pickle.load(f)\n",
    "        \n",
    "        for model_name in model_names:\n",
    "            if (dataset_name, model_name, attack_mode) in finished:\n",
    "                print(f\"⏭️ 已完成，跳过：{dataset_name} | {model_name} | {attack_mode}\")\n",
    "                continue\n",
    "            results = []\n",
    "\n",
    "            for i, sample in enumerate(tqdm(attacked_samples, desc=f'{dataset_name} | {attack_mode} | {model_name}')):\n",
    "                original_text = sample['original_text']\n",
    "                attacked_text = sample['attacked_text']\n",
    "                label = sample['label']\n",
    "\n",
    "                messages_original = create_messages(original_text)\n",
    "                messages_attacked = create_messages(attacked_text)\n",
    "\n",
    "                result_original = run_llm(messages_original, api_key=openai_key, model_name=model_name)\n",
    "                result_attacked = run_llm(messages_attacked, api_key=openai_key, model_name=model_name)\n",
    "\n",
    "                results.append({\n",
    "                    \"id\": i,\n",
    "                    \"label\": label,\n",
    "                    \"result_original\": result_original,\n",
    "                    \"result_attacked\": result_attacked,\n",
    "                    \"original_text\": original_text,\n",
    "                    \"attacked_text\": attacked_text\n",
    "                })\n",
    "\n",
    "            # 保存为 CSV & PKL\n",
    "            df = pd.DataFrame(results)\n",
    "            df.to_csv(f'{output_dir}/{dataset_name}_{model_name}_{attack_mode}.csv', index=False)\n",
    "            df.to_pickle(f'{output_dir}/{dataset_name}_{model_name}_{attack_mode}.pkl')\n",
    "            print(f\"✅ 保存成功: {dataset_name} | {model_name} | {attack_mode}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
