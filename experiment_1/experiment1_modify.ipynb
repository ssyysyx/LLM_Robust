{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# 临时设置环境变量\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "\n",
    "# 测试请求\n",
    "response = requests.get('https://www.google.com')\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义openai的Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class OpenaiClient:\n",
    "    def __init__(self, keys=None, start_id=None, proxy=None):\n",
    "        import openai\n",
    "        from openai import OpenAI\n",
    "        \n",
    "        if isinstance(keys, str):\n",
    "            keys = [keys]\n",
    "        if keys is None:\n",
    "            raise \"Please provide OpenAI Key.\"\n",
    "\n",
    "        self.key = keys\n",
    "        self.key_id = start_id or 0\n",
    "        self.key_id = self.key_id % len(self.key)\n",
    "        self.api_key = self.key[self.key_id % len(self.key)]\n",
    "        # 下面这一行base_url=\"https://api.gpts.vin/v1\"是我自己加的\n",
    "        # self.client = OpenAI(base_url=\"https://uiuiapi.com/v1\", api_key=self.api_key)\n",
    "        self.client = OpenAI(api_key=self.api_key)\n",
    "\n",
    "    def chat(self, *args, return_text=False, reduce_length=False, **kwargs):\n",
    "        while True:\n",
    "            try:\n",
    "                completion = self.client.chat.completions.create(*args, **kwargs, timeout=30)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                if \"This model's maximum context length is\" in str(e):\n",
    "                    print('reduce_length')\n",
    "                    return 'ERROR::reduce_length'\n",
    "                time.sleep(0.1)\n",
    "        if return_text:\n",
    "            completion = completion.choices[0].message.content\n",
    "        return completion\n",
    "\n",
    "    def text(self, *args, return_text=False, reduce_length=False, **kwargs):\n",
    "        while True:\n",
    "            try:\n",
    "                completion = self.client.completions.create(\n",
    "                    *args, **kwargs\n",
    "                )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                if \"This model's maximum context length is\" in str(e):\n",
    "                    print('reduce_length')\n",
    "                    return 'ERROR::reduce_length'\n",
    "                time.sleep(0.1)\n",
    "        if return_text:\n",
    "            completion = completion.choices[0].text\n",
    "        return completion\n",
    "\n",
    "def run_llm(messages, api_key=None, model_name=\"gpt-3.5-turbo\"):\n",
    "    if 'gpt' in model_name:\n",
    "        Client = OpenaiClient\n",
    "    # elif 'o1' in model_name:\n",
    "    #     Client = OpenaiClient_o1\n",
    "    # elif 'claude' in model_name:\n",
    "    #     Client = ClaudeClient\n",
    "    # elif 'gemini' in model_name:\n",
    "    #     Client = GeminiClient\n",
    "    # elif 'moonshot' in model_name:\n",
    "    #     Client = KimiClient\n",
    "    # elif 'deepseek' in model_name:\n",
    "    #     Client = BailianClient\n",
    "    # else:\n",
    "    #     Client = LitellmClient\n",
    "\n",
    "    agent = Client(api_key)\n",
    "    response = agent.chat(model=model_name, messages=messages, temperature=0, return_text=True) #temperature used to be 0\n",
    "    # print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_messages(text):\n",
    "    messages = [\n",
    "        {'role': 'system',\n",
    "         'content': \"You are a helpful assistant specialized in sentiment analysis.\"},\n",
    "        {'role': 'user',\n",
    "         'content': f\"Please classify the sentiment of the following review:\\n\\n\\\"{text}\\\"\\nRespond **only** with 'positive' or 'negative'. Do not include any extra information.\"}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集 导入举例\n",
    "imdb:25000条\n",
    "yelp:38000条\n",
    "sst:2210条  \n",
    "label:0 negative label:1 positive\n",
    "\n",
    "数据集报错可能是缺文件fancyzhx/yelp/plain_text的train\n",
    "stanfordnlp/imdb/train和unsupervised\n",
    "stanfordnlp/sst-2/data/test和train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = load_dataset(\"./stanfordnlp/imdb\")\n",
    "# dataset = load_dataset(\"./fancyzhx/yelp\")\n",
    "# dataset = load_dataset(\"./stanfordnlp/sst-2\")\n",
    "# test_dataset = dataset[\"test\"]\n",
    "# test_dataset[12500][\"text\"]\n",
    "# print(test_dataset[12500][\"label\"])\n",
    "\n",
    "# dataset = load_dataset(\"./stanfordnlp/imdb\", split=\"test[:100]\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义字符级攻击函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def text_attack(text, \n",
    "                                          perturb_prob=1.0, \n",
    "                                          max_alterations=1000, \n",
    "                                          max_per_word=2, \n",
    "                                          p_delete=0.33, \n",
    "                                          p_replace=0.33, \n",
    "                                          p_insert=0.34):\n",
    "    \"\"\"\n",
    "    执行字符级扰动并返回统计信息（安全版本，避免死循环）\n",
    "    - text：输入文本\n",
    "    - perturb_prob：每个词被扰动的概率\n",
    "    - max_alterations：全局最大扰动次数\n",
    "    - max_per_word：每个词最多扰动次数\n",
    "    - p_delete / p_replace / p_insert：三类扰动的概率分布（总和应为1.0）\n",
    "    \n",
    "    返回：\n",
    "    - 扰动后的文本\n",
    "    - 字典形式的扰动统计信息\n",
    "    \"\"\"\n",
    "    assert abs(p_delete + p_replace + p_insert - 1.0) < 1e-5, \"扰动概率之和必须为 1.0\"\n",
    "\n",
    "    words = text.split()\n",
    "    attacked_words = []\n",
    "    alterations = 0\n",
    "\n",
    "    # 扰动类型统计\n",
    "    count_delete, count_replace, count_insert = 0, 0, 0\n",
    "\n",
    "    for word in words:\n",
    "        if alterations >= max_alterations or random.random() > perturb_prob:\n",
    "            attacked_words.append(word)\n",
    "            continue\n",
    "\n",
    "        # 特殊处理：只允许删除时避免词长为1死循环\n",
    "        if len(word) <= 1 and p_delete == 1.0:\n",
    "            attacked_words.append(word)\n",
    "            continue\n",
    "\n",
    "        word_list = list(word)\n",
    "        local_alter = 0\n",
    "        loop_counter = 0\n",
    "        max_loops = 20  # 防止个别词进入死循环\n",
    "\n",
    "        while alterations < max_alterations and local_alter < max_per_word and len(word_list) > 0:\n",
    "            loop_counter += 1\n",
    "            if loop_counter > max_loops:\n",
    "                print(f\"⚠️ 跳出 `{word}` 的扰动循环，避免死循环\")\n",
    "                break\n",
    "\n",
    "            perturb_type = random.choices(\n",
    "                population=[\"delete\", \"replace\", \"insert\"],\n",
    "                weights=[p_delete, p_replace, p_insert],\n",
    "                k=1\n",
    "            )[0]\n",
    "\n",
    "            if perturb_type == \"delete\" and len(word_list) > 1:\n",
    "                idx = random.randrange(len(word_list))\n",
    "                word_list.pop(idx)\n",
    "                alterations += 1\n",
    "                local_alter += 1\n",
    "                count_delete += 1\n",
    "\n",
    "            elif perturb_type == \"replace\" and len(word_list) > 0:\n",
    "                idx = random.randrange(len(word_list))\n",
    "                word_list[idx] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "                alterations += 1\n",
    "                local_alter += 1\n",
    "                count_replace += 1\n",
    "\n",
    "            elif perturb_type == \"insert\":\n",
    "                idx = random.randint(0, len(word_list))\n",
    "                word_list.insert(idx, random.choice('abcdefghijklmnopqrstuvwxyz'))\n",
    "                alterations += 1\n",
    "                local_alter += 1\n",
    "                count_insert += 1\n",
    "\n",
    "        attacked_words.append(\"\".join(word_list))\n",
    "\n",
    "    stats = {\n",
    "        \"total_alterations\": alterations,\n",
    "        \"delete\": count_delete,\n",
    "        \"replace\": count_replace,\n",
    "        \"insert\": count_insert\n",
    "    }\n",
    "\n",
    "    return \" \".join(attacked_words), stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# 加载数据集\n",
    "# dataset_name = \"./stanfordnlp/imdb\"\n",
    "# dataset_name = \"./fancyzhx/yelp\"\n",
    "# dataset_name = \"./stanfordnlp/sst-2\"\n",
    "dataset = load_dataset(dataset_name, split=\"validation\")\n",
    "dataset = list(dataset)\n",
    "dataset = random.sample(dataset, 278)\n",
    "\n",
    "with open(\"./dataset_278/sst-2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据集、且预生成扰动文本，缓存下来:imdb, yelp, sst-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_settings = [\n",
    "    {\"name\": \"delete\", \"p_delete\": 1.0, \"p_replace\": 0.0, \"p_insert\": 0.0},\n",
    "    {\"name\": \"replace\", \"p_delete\": 0.0, \"p_replace\": 1.0, \"p_insert\": 0.0},\n",
    "    {\"name\": \"insert\", \"p_delete\": 0.0, \"p_replace\": 0.0, \"p_insert\": 1.0},\n",
    "    {\"name\": \"mix\", \"p_delete\": 0.33, \"p_replace\": 0.34, \"p_insert\": 0.33},\n",
    "]\n",
    "\n",
    "# 设置随机种子，保证攻击文本一致\n",
    "random.seed(42)\n",
    "\n",
    "# 加载数据集\n",
    "datasets = [\"imdb\", \"yelp\", \"sst-2\"]\n",
    "perturb_prob = 0.15\n",
    "\n",
    "# 每个数据集对应的文本字段名\n",
    "field_map = {\n",
    "    \"imdb\": \"text\",\n",
    "    \"yelp\": \"text\",\n",
    "    \"sst-2\": \"sentence\"\n",
    "}\n",
    "\n",
    "# 从 pickle 文件中加载回来\n",
    "for dataset_name in datasets:\n",
    "    with open(f\"./dataset_278/{dataset_name}.pkl\", \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "        # dataset = dataset[0:1]\n",
    "        \n",
    "    text_field = field_map[dataset_name]\n",
    "\n",
    "    for attack_config in attack_settings:\n",
    "        setting_name = attack_config[\"name\"]\n",
    "        p_delete = attack_config[\"p_delete\"]\n",
    "        p_replace = attack_config[\"p_replace\"]\n",
    "        p_insert = attack_config[\"p_insert\"]\n",
    "\n",
    "        # Step 1: 预生成扰动文本，缓存下来\n",
    "        attacked_samples = []\n",
    "        for item in dataset:\n",
    "            original_text = item[text_field]\n",
    "            attacked_text = text_attack(\n",
    "                original_text,\n",
    "                perturb_prob=perturb_prob,\n",
    "                max_alterations=1000,\n",
    "                max_per_word=2,\n",
    "                p_delete=p_delete,\n",
    "                p_replace=p_replace,\n",
    "                p_insert=p_insert\n",
    "            )\n",
    "            attacked_samples.append({\n",
    "                \"label\": item[\"label\"],\n",
    "                \"original_text\": original_text,\n",
    "                \"attacked_text\": attacked_text\n",
    "            })\n",
    "\n",
    "        print(attacked_samples)\n",
    "        print(len(attacked_samples))\n",
    "        # 保存扰动样本为 pickle 文件\n",
    "        save_path = f'./text_processed_278/{dataset_name}_{setting_name}_{perturb_prob}.pkl'\n",
    "\n",
    "        # 确保目标文件夹存在\n",
    "        import os\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(attacked_samples, f)\n",
    "\n",
    "        import pandas as pd\n",
    "        # 保存为 CSV 文件\n",
    "        df = pd.DataFrame(attacked_samples)\n",
    "        df.to_csv(f'./text_processed_278/{dataset_name}_{setting_name}_{perturb_prob}.csv', index=False)  # 保存为 CSV\n",
    "\n",
    "        print(f\"✅ 已保存：{save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM 判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏭️ 已完成，跳过：imdb | gpt-3.5-turbo | delete | 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | delete | gpt-4-turbo | 0.75: 100%|██████████| 278/278 [22:53<00:00,  4.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4-turbo | delete | 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | delete | gpt-4o | 0.75: 100%|██████████| 278/278 [19:01<00:00,  4.11s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4o | delete | 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | delete | gpt-3.5-turbo | 0.55: 100%|██████████| 278/278 [18:15<00:00,  3.94s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-3.5-turbo | delete | 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | delete | gpt-4-turbo | 0.55: 100%|██████████| 278/278 [23:12<00:00,  5.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4-turbo | delete | 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | delete | gpt-4o | 0.55: 100%|██████████| 278/278 [18:39<00:00,  4.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4o | delete | 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | delete | gpt-3.5-turbo | 0.35: 100%|██████████| 278/278 [18:09<00:00,  3.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-3.5-turbo | delete | 0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | delete | gpt-4-turbo | 0.35: 100%|██████████| 278/278 [22:25<00:00,  4.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4-turbo | delete | 0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | delete | gpt-4o | 0.35: 100%|██████████| 278/278 [18:13<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4o | delete | 0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | delete | gpt-3.5-turbo | 0.15: 100%|██████████| 278/278 [17:49<00:00,  3.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-3.5-turbo | delete | 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | delete | gpt-4-turbo | 0.15: 100%|██████████| 278/278 [23:25<00:00,  5.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4-turbo | delete | 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | delete | gpt-4o | 0.15: 100%|██████████| 278/278 [18:25<00:00,  3.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4o | delete | 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | replace | gpt-3.5-turbo | 0.75: 100%|██████████| 278/278 [17:38<00:00,  3.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-3.5-turbo | replace | 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | replace | gpt-4-turbo | 0.75: 100%|██████████| 278/278 [23:15<00:00,  5.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4-turbo | replace | 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | replace | gpt-4o | 0.75: 100%|██████████| 278/278 [18:06<00:00,  3.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4o | replace | 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | replace | gpt-3.5-turbo | 0.55: 100%|██████████| 278/278 [17:58<00:00,  3.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-3.5-turbo | replace | 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | replace | gpt-4-turbo | 0.55: 100%|██████████| 278/278 [22:30<00:00,  4.86s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4-turbo | replace | 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | replace | gpt-4o | 0.55: 100%|██████████| 278/278 [19:16<00:00,  4.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4o | replace | 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | replace | gpt-3.5-turbo | 0.35: 100%|██████████| 278/278 [17:46<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-3.5-turbo | replace | 0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | replace | gpt-4-turbo | 0.35: 100%|██████████| 278/278 [21:55<00:00,  4.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4-turbo | replace | 0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | replace | gpt-4o | 0.35: 100%|██████████| 278/278 [18:28<00:00,  3.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4o | replace | 0.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | replace | gpt-3.5-turbo | 0.15: 100%|██████████| 278/278 [17:44<00:00,  3.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-3.5-turbo | replace | 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | replace | gpt-4-turbo | 0.15: 100%|██████████| 278/278 [22:00<00:00,  4.75s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4-turbo | replace | 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | replace | gpt-4o | 0.15: 100%|██████████| 278/278 [18:21<00:00,  3.96s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4o | replace | 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | insert | gpt-3.5-turbo | 0.75: 100%|██████████| 278/278 [17:25<00:00,  3.76s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-3.5-turbo | insert | 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | insert | gpt-4-turbo | 0.75: 100%|██████████| 278/278 [21:24<00:00,  4.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4-turbo | insert | 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | insert | gpt-4o | 0.75: 100%|██████████| 278/278 [19:38<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-4o | insert | 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | insert | gpt-3.5-turbo | 0.55: 100%|██████████| 278/278 [18:54<00:00,  4.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 保存成功: imdb | gpt-3.5-turbo | insert | 0.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "imdb | insert | gpt-4-turbo | 0.55:   0%|          | 0/278 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 45\u001b[0m\n\u001b[0;32m     42\u001b[0m messages_original \u001b[38;5;241m=\u001b[39m create_messages(original_text)\n\u001b[0;32m     43\u001b[0m messages_attacked \u001b[38;5;241m=\u001b[39m create_messages(attacked_text)\n\u001b[1;32m---> 45\u001b[0m result_original \u001b[38;5;241m=\u001b[39m \u001b[43mrun_llm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages_original\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopenai_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m result_attacked \u001b[38;5;241m=\u001b[39m run_llm(messages_attacked, api_key\u001b[38;5;241m=\u001b[39mopenai_key, model_name\u001b[38;5;241m=\u001b[39mmodel_name)\n\u001b[0;32m     48\u001b[0m results\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m     49\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: i,\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m: label,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattacked_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: attacked_text\n\u001b[0;32m     55\u001b[0m })\n",
      "Cell \u001b[1;32mIn[2], line 69\u001b[0m, in \u001b[0;36mrun_llm\u001b[1;34m(messages, api_key, model_name)\u001b[0m\n\u001b[0;32m     55\u001b[0m     Client \u001b[38;5;241m=\u001b[39m OpenaiClient\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m# elif 'o1' in model_name:\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#     Client = OpenaiClient_o1\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# elif 'claude' in model_name:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m#     Client = LitellmClient\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m agent \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m response \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchat(model\u001b[38;5;241m=\u001b[39mmodel_name, messages\u001b[38;5;241m=\u001b[39mmessages, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, return_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m#temperature used to be 0\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(response)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 19\u001b[0m, in \u001b[0;36mOpenaiClient.__init__\u001b[1;34m(self, keys, start_id, proxy)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_id \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey)]\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# 下面这一行base_url=\"https://api.gpts.vin/v1\"是我自己加的\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# self.client = OpenAI(base_url=\"https://uiuiapi.com/v1\", api_key=self.api_key)\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m \u001b[43mOpenAI\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ssyysyx\\.conda\\envs\\lmql\\lib\\site-packages\\openai\\_client.py:130\u001b[0m, in \u001b[0;36mOpenAI.__init__\u001b[1;34m(self, api_key, organization, project, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m base_url \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    128\u001b[0m     base_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.openai.com/v1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m__version__\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbase_url\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_url\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    134\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp_client\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_client\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    136\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_query\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_strict_response_validation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_strict_response_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_stream_cls \u001b[38;5;241m=\u001b[39m Stream\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompletions \u001b[38;5;241m=\u001b[39m completions\u001b[38;5;241m.\u001b[39mCompletions(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ssyysyx\\.conda\\envs\\lmql\\lib\\site-packages\\openai\\_base_client.py:870\u001b[0m, in \u001b[0;36mSyncAPIClient.__init__\u001b[1;34m(self, version, base_url, max_retries, timeout, transport, proxies, limits, http_client, custom_headers, custom_query, _strict_response_validation)\u001b[0m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    854\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid `http_client` argument; Expected an instance of `httpx.Client` but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(http_client)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    855\u001b[0m     )\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    858\u001b[0m     version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[0;32m    859\u001b[0m     limits\u001b[38;5;241m=\u001b[39mlimits,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    868\u001b[0m     _strict_response_validation\u001b[38;5;241m=\u001b[39m_strict_response_validation,\n\u001b[0;32m    869\u001b[0m )\n\u001b[1;32m--> 870\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client \u001b[38;5;241m=\u001b[39m http_client \u001b[38;5;129;01mor\u001b[39;00m SyncHttpxClientWrapper(\n\u001b[0;32m    871\u001b[0m     base_url\u001b[38;5;241m=\u001b[39mbase_url,\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;66;03m# cast to a valid type because mypy doesn't understand our type narrowing\u001b[39;00m\n\u001b[0;32m    873\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mcast(Timeout, timeout),\n\u001b[0;32m    874\u001b[0m     limits\u001b[38;5;241m=\u001b[39mlimits,\n\u001b[0;32m    875\u001b[0m     follow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[0;32m    877\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ssyysyx\\.conda\\envs\\lmql\\lib\\site-packages\\openai\\_base_client.py:762\u001b[0m, in \u001b[0;36m_DefaultHttpxClient.__init__\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    760\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlimits\u001b[39m\u001b[38;5;124m\"\u001b[39m, DEFAULT_CONNECTION_LIMITS)\n\u001b[0;32m    761\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfollow_redirects\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 762\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ssyysyx\\.conda\\envs\\lmql\\lib\\site-packages\\httpx\\_client.py:695\u001b[0m, in \u001b[0;36mClient.__init__\u001b[1;34m(self, auth, params, headers, cookies, verify, cert, http1, http2, proxy, proxies, mounts, timeout, follow_redirects, limits, max_redirects, event_hooks, base_url, transport, app, trust_env, default_encoding)\u001b[0m\n\u001b[0;32m    683\u001b[0m proxy_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_proxy_map(proxies \u001b[38;5;129;01mor\u001b[39;00m proxy, allow_env_proxies)\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_transport(\n\u001b[0;32m    686\u001b[0m     verify\u001b[38;5;241m=\u001b[39mverify,\n\u001b[0;32m    687\u001b[0m     cert\u001b[38;5;241m=\u001b[39mcert,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    693\u001b[0m     trust_env\u001b[38;5;241m=\u001b[39mtrust_env,\n\u001b[0;32m    694\u001b[0m )\n\u001b[1;32m--> 695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mounts: \u001b[38;5;28mdict\u001b[39m[URLPattern, BaseTransport \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    696\u001b[0m     URLPattern(key): \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proxy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    698\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_proxy_transport(\n\u001b[0;32m    699\u001b[0m         proxy,\n\u001b[0;32m    700\u001b[0m         verify\u001b[38;5;241m=\u001b[39mverify,\n\u001b[0;32m    701\u001b[0m         cert\u001b[38;5;241m=\u001b[39mcert,\n\u001b[0;32m    702\u001b[0m         http1\u001b[38;5;241m=\u001b[39mhttp1,\n\u001b[0;32m    703\u001b[0m         http2\u001b[38;5;241m=\u001b[39mhttp2,\n\u001b[0;32m    704\u001b[0m         limits\u001b[38;5;241m=\u001b[39mlimits,\n\u001b[0;32m    705\u001b[0m         trust_env\u001b[38;5;241m=\u001b[39mtrust_env,\n\u001b[0;32m    706\u001b[0m     )\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, proxy \u001b[38;5;129;01min\u001b[39;00m proxy_map\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    708\u001b[0m }\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mounts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    710\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mounts\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    711\u001b[0m         {URLPattern(key): transport \u001b[38;5;28;01mfor\u001b[39;00m key, transport \u001b[38;5;129;01min\u001b[39;00m mounts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    712\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ssyysyx\\.conda\\envs\\lmql\\lib\\site-packages\\httpx\\_client.py:698\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    683\u001b[0m proxy_map \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_proxy_map(proxies \u001b[38;5;129;01mor\u001b[39;00m proxy, allow_env_proxies)\n\u001b[0;32m    685\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transport \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_transport(\n\u001b[0;32m    686\u001b[0m     verify\u001b[38;5;241m=\u001b[39mverify,\n\u001b[0;32m    687\u001b[0m     cert\u001b[38;5;241m=\u001b[39mcert,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    693\u001b[0m     trust_env\u001b[38;5;241m=\u001b[39mtrust_env,\n\u001b[0;32m    694\u001b[0m )\n\u001b[0;32m    695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mounts: \u001b[38;5;28mdict\u001b[39m[URLPattern, BaseTransport \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    696\u001b[0m     URLPattern(key): \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    697\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proxy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 698\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_proxy_transport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhttp1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhttp2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, proxy \u001b[38;5;129;01min\u001b[39;00m proxy_map\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    708\u001b[0m }\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mounts \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    710\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mounts\u001b[38;5;241m.\u001b[39mupdate(\n\u001b[0;32m    711\u001b[0m         {URLPattern(key): transport \u001b[38;5;28;01mfor\u001b[39;00m key, transport \u001b[38;5;129;01min\u001b[39;00m mounts\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m    712\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ssyysyx\\.conda\\envs\\lmql\\lib\\site-packages\\httpx\\_client.py:752\u001b[0m, in \u001b[0;36mClient._init_proxy_transport\u001b[1;34m(self, proxy, verify, cert, http1, http2, limits, trust_env)\u001b[0m\n\u001b[0;32m    742\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_init_proxy_transport\u001b[39m(\n\u001b[0;32m    743\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    744\u001b[0m     proxy: Proxy,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    750\u001b[0m     trust_env: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    751\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseTransport:\n\u001b[1;32m--> 752\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHTTPTransport\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhttp1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhttp2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlimits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    760\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ssyysyx\\.conda\\envs\\lmql\\lib\\site-packages\\httpx\\_transports\\default.py:136\u001b[0m, in \u001b[0;36mHTTPTransport.__init__\u001b[1;34m(self, verify, cert, http1, http2, limits, trust_env, proxy, uds, local_address, retries, socket_options)\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    124\u001b[0m     verify: VerifyTypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    134\u001b[0m     socket_options: typing\u001b[38;5;241m.\u001b[39mIterable[SOCKET_OPTION] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    135\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 136\u001b[0m     ssl_context \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_ssl_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m     proxy \u001b[38;5;241m=\u001b[39m Proxy(url\u001b[38;5;241m=\u001b[39mproxy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(proxy, (\u001b[38;5;28mstr\u001b[39m, URL)) \u001b[38;5;28;01melse\u001b[39;00m proxy\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m proxy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ssyysyx\\.conda\\envs\\lmql\\lib\\site-packages\\httpx\\_config.py:53\u001b[0m, in \u001b[0;36mcreate_ssl_context\u001b[1;34m(cert, verify, trust_env, http2)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_ssl_context\u001b[39m(\n\u001b[0;32m     48\u001b[0m     cert: CertTypes \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     49\u001b[0m     verify: VerifyTypes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     50\u001b[0m     trust_env: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     51\u001b[0m     http2: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     52\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ssl\u001b[38;5;241m.\u001b[39mSSLContext:\n\u001b[1;32m---> 53\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSSLConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhttp2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp2\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mssl_context\n",
      "File \u001b[1;32mc:\\Users\\ssyysyx\\.conda\\envs\\lmql\\lib\\site-packages\\httpx\\_config.py:77\u001b[0m, in \u001b[0;36mSSLConfig.__init__\u001b[1;34m(self, cert, verify, trust_env, http2)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrust_env \u001b[38;5;241m=\u001b[39m trust_env\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp2 \u001b[38;5;241m=\u001b[39m http2\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_ssl_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ssyysyx\\.conda\\envs\\lmql\\lib\\site-packages\\httpx\\_config.py:89\u001b[0m, in \u001b[0;36mSSLConfig.load_ssl_context\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     80\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_ssl_context verify=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m cert=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m trust_env=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m http2=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp2,\n\u001b[0;32m     86\u001b[0m )\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify:\n\u001b[1;32m---> 89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_ssl_context_verify\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_ssl_context_no_verify()\n",
      "File \u001b[1;32mc:\\Users\\ssyysyx\\.conda\\envs\\lmql\\lib\\site-packages\\httpx\\_config.py:147\u001b[0m, in \u001b[0;36mSSLConfig.load_ssl_context_verify\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    145\u001b[0m     cafile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(ca_bundle_path)\n\u001b[0;32m    146\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mload_verify_locations cafile=\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, cafile)\n\u001b[1;32m--> 147\u001b[0m     \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_verify_locations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcafile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcafile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ca_bundle_path\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[0;32m    149\u001b[0m     capath \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(ca_bundle_path)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 已完成的组合\n",
    "finished = {\n",
    "    ('imdb', 'gpt-3.5-turbo', 'delete', 0.75),\n",
    "}\n",
    "\n",
    "import openai\n",
    "openai_key = 'sk-' # 补全\n",
    "model_names = ['gpt-3.5-turbo', 'gpt-4-turbo','gpt-4o']\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# 模型名 & 数据集 & 攻击模式 & 扰乱概率\n",
    "model_names = ['gpt-3.5-turbo', 'gpt-4-turbo', 'gpt-4o']\n",
    "datasets = ['imdb', 'yelp', 'sst-2']\n",
    "attack_modes = ['delete', 'replace', 'insert', 'mix']\n",
    "perturb_probs = [0.75, 0.55, 0.35, 0.15]\n",
    "\n",
    "# 输出路径\n",
    "output_dir = './result_278'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    for attack_mode in attack_modes:\n",
    "        for perturb_prob in perturb_probs:\n",
    "            # 加载攻击样本\n",
    "            file_path = f'./text_processed_278/{dataset_name}_{attack_mode}_{perturb_prob}.pkl'\n",
    "            with open(file_path, 'rb') as f:\n",
    "                attacked_samples = pickle.load(f)\n",
    "            \n",
    "            for model_name in model_names:\n",
    "                if (dataset_name, model_name, attack_mode, perturb_prob) in finished:\n",
    "                    print(f\"⏭️ 已完成，跳过：{dataset_name} | {model_name} | {attack_mode} | {perturb_prob}\")\n",
    "                    continue\n",
    "                results = []\n",
    "\n",
    "                for i, sample in enumerate(tqdm(attacked_samples, desc=f'{dataset_name} | {attack_mode} | {model_name} | {perturb_prob}')):\n",
    "                    original_text = sample['original_text']\n",
    "                    attacked_text = sample['attacked_text']\n",
    "                    label = sample['label']\n",
    "\n",
    "                    messages_original = create_messages(original_text)\n",
    "                    messages_attacked = create_messages(attacked_text)\n",
    "\n",
    "                    result_original = run_llm(messages_original, api_key=openai_key, model_name=model_name)\n",
    "                    result_attacked = run_llm(messages_attacked, api_key=openai_key, model_name=model_name)\n",
    "\n",
    "                    results.append({\n",
    "                        \"id\": i,\n",
    "                        \"label\": label,\n",
    "                        \"result_original\": result_original,\n",
    "                        \"result_attacked\": result_attacked,\n",
    "                        \"original_text\": original_text,\n",
    "                        \"attacked_text\": attacked_text\n",
    "                    })\n",
    "\n",
    "                # 保存为 CSV & PKL\n",
    "                df = pd.DataFrame(results)\n",
    "                df.to_csv(f'{output_dir}/{dataset_name}_{model_name}_{attack_mode}_{perturb_prob}.csv', index=False)\n",
    "                df.to_pickle(f'{output_dir}/{dataset_name}_{model_name}_{attack_mode}_{perturb_prob}.pkl')\n",
    "                print(f\"✅ 保存成功: {dataset_name} | {model_name} | {attack_mode} | {perturb_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改perturb_prob和max_alterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error ratio based global perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def text_attack_by_ratio(text, \n",
    "                         error_ratio=0.4,\n",
    "                         p_delete=0.33, \n",
    "                         p_replace=0.33, \n",
    "                         p_insert=0.34,\n",
    "                         max_per_word=2):\n",
    "    \"\"\"\n",
    "    随机选择单词进行字符级扰动，每个单词最多扰动 max_per_word 次，\n",
    "    整体最多扰动 N_max 次。\n",
    "\n",
    "    参数：\n",
    "      - text: 原文本\n",
    "      - error_ratio: 错误率（控制 N_max）\n",
    "      - p_delete, p_replace, p_insert: 扰动操作比例\n",
    "      - max_per_word: 每个单词最多可扰动几次\n",
    "\n",
    "    返回：\n",
    "      - attacked_text: 扰动后的文本\n",
    "      - stats: 扰动统计信息\n",
    "    \"\"\"\n",
    "    # 拆分单词\n",
    "    words = text.split()\n",
    "    N = sum(len(w) for w in words)\n",
    "    if N == 0:\n",
    "        return text, {\n",
    "            \"total_alterations\": 0,\n",
    "            \"delete\": 0,\n",
    "            \"replace\": 0,\n",
    "            \"insert\": 0\n",
    "        }\n",
    "\n",
    "    # 计算最大总扰动次数\n",
    "    N_max = int(round(error_ratio * N))\n",
    "    if N_max <= 0:\n",
    "        return text, {\n",
    "            \"total_alterations\": 0,\n",
    "            \"delete\": 0,\n",
    "            \"replace\": 0,\n",
    "            \"insert\": 0\n",
    "        }\n",
    "\n",
    "    # 将单词转换为字符列表，记录每个单词已修改次数\n",
    "    word_chars_list = [list(w) for w in words]\n",
    "    word_alter_counts = [0] * len(words)\n",
    "\n",
    "    # 统计信息\n",
    "    total_alterations = 0\n",
    "    count_delete, count_replace, count_insert = 0, 0, 0\n",
    "\n",
    "    attempts = 0\n",
    "    max_attempts = 10000  # 防止死循环\n",
    "\n",
    "    while total_alterations < N_max and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "\n",
    "        # 随机选一个单词\n",
    "        idx = random.randrange(len(words))\n",
    "\n",
    "        # 如果该单词已达最大修改次数，则跳过\n",
    "        if word_alter_counts[idx] >= max_per_word:\n",
    "            continue\n",
    "\n",
    "        chars = word_chars_list[idx]\n",
    "\n",
    "        # 判断是否还能执行 delete/replace 操作\n",
    "        available_ops = []\n",
    "        if len(chars) > 0:\n",
    "            available_ops.extend([\"delete\", \"replace\"])\n",
    "        available_ops.append(\"insert\")\n",
    "\n",
    "        # 在可用的操作中按权重选择操作\n",
    "        weights = []\n",
    "        if \"delete\" in available_ops: weights.append(p_delete)\n",
    "        if \"replace\" in available_ops: weights.append(p_replace)\n",
    "        if \"insert\" in available_ops: weights.append(p_insert)\n",
    "\n",
    "        op_type = random.choices(\n",
    "            population=available_ops,\n",
    "            weights=weights,\n",
    "            k=1\n",
    "        )[0]\n",
    "\n",
    "        if op_type == \"delete\":\n",
    "            idx_char = random.randrange(len(chars))\n",
    "            chars.pop(idx_char)\n",
    "            count_delete += 1\n",
    "        elif op_type == \"replace\":\n",
    "            idx_char = random.randrange(len(chars))\n",
    "            chars[idx_char] = random.choice(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "            count_replace += 1\n",
    "        elif op_type == \"insert\":\n",
    "            idx_char = random.randint(0, len(chars))\n",
    "            chars.insert(idx_char, random.choice(\"abcdefghijklmnopqrstuvwxyz\"))\n",
    "            count_insert += 1\n",
    "\n",
    "        word_alter_counts[idx] += 1\n",
    "        total_alterations += 1\n",
    "\n",
    "    # 拼回文本\n",
    "    attacked_words = [\"\".join(c) for c in word_chars_list]\n",
    "    attacked_text = \" \".join(attacked_words)\n",
    "\n",
    "    stats = {\n",
    "        \"total_alterations\": total_alterations,\n",
    "        \"delete\": count_delete,\n",
    "        \"replace\": count_replace,\n",
    "        \"insert\": count_insert\n",
    "    }\n",
    "    return attacked_text, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "attack_settings = [\n",
    "    # {\"name\": \"delete\", \"p_delete\": 1.0, \"p_replace\": 0.0, \"p_insert\": 0.0},\n",
    "    # {\"name\": \"replace\", \"p_delete\": 0.0, \"p_replace\": 1.0, \"p_insert\": 0.0},\n",
    "    # {\"name\": \"insert\", \"p_delete\": 0.0, \"p_replace\": 0.0, \"p_insert\": 1.0},\n",
    "    {\"name\": \"mix\", \"p_delete\": 0.33, \"p_replace\": 0.34, \"p_insert\": 0.33},\n",
    "]\n",
    "error_ratio = 0.4\n",
    "\n",
    "# 设置随机种子，保证攻击文本一致\n",
    "random.seed(42)\n",
    "\n",
    "# 加载数据集\n",
    "# datasets = [\"imdb\", \"yelp\"]\n",
    "datasets = [\"sst-2\"]\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    with open(f\"./dataset_278/{dataset_name}.pkl\", \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "        # dataset = dataset[0:1]  # 可解注用于调试小样本\n",
    "\n",
    "    for attack_config in attack_settings:\n",
    "        setting_name = attack_config[\"name\"]\n",
    "        p_delete = attack_config[\"p_delete\"]\n",
    "        p_replace = attack_config[\"p_replace\"]\n",
    "        p_insert = attack_config[\"p_insert\"]\n",
    "\n",
    "        # Step 1: 预生成扰动文本，缓存下来\n",
    "        attacked_samples = []\n",
    "        for item in dataset:\n",
    "            original_text = item[\"sentence\"]\n",
    "            # original_text = item[\"text\"]  # 如果字段名是 text，请改为此行\n",
    "\n",
    "            attacked_text, stats = text_attack_by_ratio(\n",
    "                original_text,\n",
    "                error_ratio=error_ratio,\n",
    "                p_delete=p_delete,\n",
    "                p_replace=p_replace,\n",
    "                p_insert=p_insert,\n",
    "                max_per_word=2,\n",
    "            )\n",
    "            attacked_samples.append({\n",
    "                \"label\": item[\"label\"],\n",
    "                \"original_text\": original_text,\n",
    "                \"attacked_text\": attacked_text,\n",
    "                \"status\": stats\n",
    "            })\n",
    "\n",
    "        print(attacked_samples[:3])  # 只打印前3条检查格式\n",
    "        print(f\"总样本数: {len(attacked_samples)}\")\n",
    "\n",
    "        # 保存扰动样本为 pickle 文件\n",
    "        save_path = f'./ratio_278/{dataset_name}_{setting_name}_{error_ratio}.pkl'\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(attacked_samples, f)\n",
    "\n",
    "        # 保存为 CSV 文件\n",
    "        df = pd.DataFrame(attacked_samples)\n",
    "        df.to_csv(f'./ratio_278/{dataset_name}_{setting_name}_{error_ratio}.csv', index=False)\n",
    "\n",
    "        print(f\"✅ 已保存：{save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
