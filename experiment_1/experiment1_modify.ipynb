{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置代理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "# 临时设置环境变量\n",
    "os.environ['http_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['https_proxy'] = 'http://127.0.0.1:7890'\n",
    "os.environ['all_proxy'] = 'socks5://127.0.0.1:7890'\n",
    "\n",
    "# 测试请求\n",
    "response = requests.get('https://www.google.com')\n",
    "print(response.status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义openai的Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "class OpenaiClient:\n",
    "    def __init__(self, keys=None, start_id=None, proxy=None):\n",
    "        import openai\n",
    "        from openai import OpenAI\n",
    "        \n",
    "        if isinstance(keys, str):\n",
    "            keys = [keys]\n",
    "        if keys is None:\n",
    "            raise \"Please provide OpenAI Key.\"\n",
    "\n",
    "        self.key = keys\n",
    "        self.key_id = start_id or 0\n",
    "        self.key_id = self.key_id % len(self.key)\n",
    "        self.api_key = self.key[self.key_id % len(self.key)]\n",
    "        # 下面这一行base_url=\"https://api.gpts.vin/v1\"是我自己加的\n",
    "        # self.client = OpenAI(base_url=\"https://uiuiapi.com/v1\", api_key=self.api_key)\n",
    "        self.client = OpenAI(api_key=self.api_key)\n",
    "\n",
    "    def chat(self, *args, return_text=False, reduce_length=False, **kwargs):\n",
    "        while True:\n",
    "            try:\n",
    "                completion = self.client.chat.completions.create(*args, **kwargs, timeout=30)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(str(e))\n",
    "                if \"This model's maximum context length is\" in str(e):\n",
    "                    print('reduce_length')\n",
    "                    return 'ERROR::reduce_length'\n",
    "                time.sleep(0.1)\n",
    "        if return_text:\n",
    "            completion = completion.choices[0].message.content\n",
    "        return completion\n",
    "\n",
    "    def text(self, *args, return_text=False, reduce_length=False, **kwargs):\n",
    "        while True:\n",
    "            try:\n",
    "                completion = self.client.completions.create(\n",
    "                    *args, **kwargs\n",
    "                )\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                if \"This model's maximum context length is\" in str(e):\n",
    "                    print('reduce_length')\n",
    "                    return 'ERROR::reduce_length'\n",
    "                time.sleep(0.1)\n",
    "        if return_text:\n",
    "            completion = completion.choices[0].text\n",
    "        return completion\n",
    "\n",
    "def run_llm(messages, api_key=None, model_name=\"gpt-3.5-turbo\"):\n",
    "    if 'gpt' in model_name:\n",
    "        Client = OpenaiClient\n",
    "    # elif 'o1' in model_name:\n",
    "    #     Client = OpenaiClient_o1\n",
    "    # elif 'claude' in model_name:\n",
    "    #     Client = ClaudeClient\n",
    "    # elif 'gemini' in model_name:\n",
    "    #     Client = GeminiClient\n",
    "    # elif 'moonshot' in model_name:\n",
    "    #     Client = KimiClient\n",
    "    # elif 'deepseek' in model_name:\n",
    "    #     Client = BailianClient\n",
    "    # else:\n",
    "    #     Client = LitellmClient\n",
    "\n",
    "    agent = Client(api_key)\n",
    "    response = agent.chat(model=model_name, messages=messages, temperature=0, return_text=True) #temperature used to be 0\n",
    "    # print(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_messages(text):\n",
    "    messages = [\n",
    "        {'role': 'system',\n",
    "         'content': \"You are a helpful assistant specialized in sentiment analysis.\"},\n",
    "        {'role': 'user',\n",
    "         'content': f\"Please classify the sentiment of the following review:\\n\\n\\\"{text}\\\"\\nRespond **only** with 'positive' or 'negative'. Do not include any extra information.\"}\n",
    "    ]\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集 导入举例\n",
    "imdb:25000条\n",
    "yelp:38000条\n",
    "sst:2210条  \n",
    "label:0 negative label:1 positive\n",
    "\n",
    "数据集报错可能是缺文件fancyzhx/yelp/plain_text的train\n",
    "stanfordnlp/imdb/train和unsupervised\n",
    "stanfordnlp/sst-2/data/test和train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the dataset\n",
    "# dataset = load_dataset(\"./stanfordnlp/imdb\")\n",
    "# dataset = load_dataset(\"./fancyzhx/yelp\")\n",
    "# dataset = load_dataset(\"./stanfordnlp/sst-2\")\n",
    "# test_dataset = dataset[\"test\"]\n",
    "# test_dataset[12500][\"text\"]\n",
    "# print(test_dataset[12500][\"label\"])\n",
    "\n",
    "# dataset = load_dataset(\"./stanfordnlp/imdb\", split=\"test[:100]\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义字符级攻击函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def text_attack(text, \n",
    "                                          perturb_prob=1.0, \n",
    "                                          max_alterations=1000, \n",
    "                                          max_per_word=2, \n",
    "                                          p_delete=0.33, \n",
    "                                          p_replace=0.33, \n",
    "                                          p_insert=0.34):\n",
    "    \"\"\"\n",
    "    执行字符级扰动并返回统计信息（安全版本，避免死循环）\n",
    "    - text：输入文本\n",
    "    - perturb_prob：每个词被扰动的概率\n",
    "    - max_alterations：全局最大扰动次数\n",
    "    - max_per_word：每个词最多扰动次数\n",
    "    - p_delete / p_replace / p_insert：三类扰动的概率分布（总和应为1.0）\n",
    "    \n",
    "    返回：\n",
    "    - 扰动后的文本\n",
    "    - 字典形式的扰动统计信息\n",
    "    \"\"\"\n",
    "    assert abs(p_delete + p_replace + p_insert - 1.0) < 1e-5, \"扰动概率之和必须为 1.0\"\n",
    "\n",
    "    words = text.split()\n",
    "    attacked_words = []\n",
    "    alterations = 0\n",
    "\n",
    "    # 扰动类型统计\n",
    "    count_delete, count_replace, count_insert = 0, 0, 0\n",
    "\n",
    "    for word in words:\n",
    "        if alterations >= max_alterations or random.random() > perturb_prob:\n",
    "            attacked_words.append(word)\n",
    "            continue\n",
    "\n",
    "        # 特殊处理：只允许删除时避免词长为1死循环\n",
    "        if len(word) <= 1 and p_delete == 1.0:\n",
    "            attacked_words.append(word)\n",
    "            continue\n",
    "\n",
    "        word_list = list(word)\n",
    "        local_alter = 0\n",
    "        loop_counter = 0\n",
    "        max_loops = 20  # 防止个别词进入死循环\n",
    "\n",
    "        while alterations < max_alterations and local_alter < max_per_word and len(word_list) > 0:\n",
    "            loop_counter += 1\n",
    "            if loop_counter > max_loops:\n",
    "                print(f\"⚠️ 跳出 `{word}` 的扰动循环，避免死循环\")\n",
    "                break\n",
    "\n",
    "            perturb_type = random.choices(\n",
    "                population=[\"delete\", \"replace\", \"insert\"],\n",
    "                weights=[p_delete, p_replace, p_insert],\n",
    "                k=1\n",
    "            )[0]\n",
    "\n",
    "            if perturb_type == \"delete\" and len(word_list) > 1:\n",
    "                idx = random.randrange(len(word_list))\n",
    "                word_list.pop(idx)\n",
    "                alterations += 1\n",
    "                local_alter += 1\n",
    "                count_delete += 1\n",
    "\n",
    "            elif perturb_type == \"replace\" and len(word_list) > 0:\n",
    "                idx = random.randrange(len(word_list))\n",
    "                word_list[idx] = random.choice('abcdefghijklmnopqrstuvwxyz')\n",
    "                alterations += 1\n",
    "                local_alter += 1\n",
    "                count_replace += 1\n",
    "\n",
    "            elif perturb_type == \"insert\":\n",
    "                idx = random.randint(0, len(word_list))\n",
    "                word_list.insert(idx, random.choice('abcdefghijklmnopqrstuvwxyz'))\n",
    "                alterations += 1\n",
    "                local_alter += 1\n",
    "                count_insert += 1\n",
    "\n",
    "        attacked_words.append(\"\".join(word_list))\n",
    "\n",
    "    stats = {\n",
    "        \"total_alterations\": alterations,\n",
    "        \"delete\": count_delete,\n",
    "        \"replace\": count_replace,\n",
    "        \"insert\": count_insert\n",
    "    }\n",
    "\n",
    "    return \" \".join(attacked_words), stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "# 加载数据集\n",
    "# dataset_name = \"./stanfordnlp/imdb\"\n",
    "# dataset_name = \"./fancyzhx/yelp\"\n",
    "# dataset_name = \"./stanfordnlp/sst-2\"\n",
    "dataset = load_dataset(dataset_name, split=\"validation\")\n",
    "dataset = list(dataset)\n",
    "dataset = random.sample(dataset, 278)\n",
    "\n",
    "with open(\"./dataset_278/sst-2.pkl\", \"wb\") as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载数据集、且预生成扰动文本，缓存下来:imdb, yelp, sst-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attack_settings = [\n",
    "    {\"name\": \"delete\", \"p_delete\": 1.0, \"p_replace\": 0.0, \"p_insert\": 0.0},\n",
    "    {\"name\": \"replace\", \"p_delete\": 0.0, \"p_replace\": 1.0, \"p_insert\": 0.0},\n",
    "    {\"name\": \"insert\", \"p_delete\": 0.0, \"p_replace\": 0.0, \"p_insert\": 1.0},\n",
    "    {\"name\": \"mix\", \"p_delete\": 0.33, \"p_replace\": 0.34, \"p_insert\": 0.33},\n",
    "]\n",
    "\n",
    "# 设置随机种子，保证攻击文本一致\n",
    "random.seed(42)\n",
    "\n",
    "# 加载数据集\n",
    "datasets = [\"imdb\", \"yelp\", \"sst-2\"]\n",
    "perturb_prob = 0.15\n",
    "\n",
    "# 每个数据集对应的文本字段名\n",
    "field_map = {\n",
    "    \"imdb\": \"text\",\n",
    "    \"yelp\": \"text\",\n",
    "    \"sst-2\": \"sentence\"\n",
    "}\n",
    "\n",
    "# 从 pickle 文件中加载回来\n",
    "for dataset_name in datasets:\n",
    "    with open(f\"./dataset_278/{dataset_name}.pkl\", \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "        # dataset = dataset[0:1]\n",
    "        \n",
    "    text_field = field_map[dataset_name]\n",
    "\n",
    "    for attack_config in attack_settings:\n",
    "        setting_name = attack_config[\"name\"]\n",
    "        p_delete = attack_config[\"p_delete\"]\n",
    "        p_replace = attack_config[\"p_replace\"]\n",
    "        p_insert = attack_config[\"p_insert\"]\n",
    "\n",
    "        # Step 1: 预生成扰动文本，缓存下来\n",
    "        attacked_samples = []\n",
    "        for item in dataset:\n",
    "            original_text = item[text_field]\n",
    "            attacked_text = text_attack(\n",
    "                original_text,\n",
    "                perturb_prob=perturb_prob,\n",
    "                max_alterations=1000,\n",
    "                max_per_word=2,\n",
    "                p_delete=p_delete,\n",
    "                p_replace=p_replace,\n",
    "                p_insert=p_insert\n",
    "            )\n",
    "            attacked_samples.append({\n",
    "                \"label\": item[\"label\"],\n",
    "                \"original_text\": original_text,\n",
    "                \"attacked_text\": attacked_text\n",
    "            })\n",
    "\n",
    "        print(attacked_samples)\n",
    "        print(len(attacked_samples))\n",
    "        # 保存扰动样本为 pickle 文件\n",
    "        save_path = f'./text_processed_278/{dataset_name}_{setting_name}_{perturb_prob}.pkl'\n",
    "\n",
    "        # 确保目标文件夹存在\n",
    "        import os\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(attacked_samples, f)\n",
    "\n",
    "        import pandas as pd\n",
    "        # 保存为 CSV 文件\n",
    "        df = pd.DataFrame(attacked_samples)\n",
    "        df.to_csv(f'./text_processed_278/{dataset_name}_{setting_name}_{perturb_prob}.csv', index=False)  # 保存为 CSV\n",
    "\n",
    "        print(f\"✅ 已保存：{save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LLM 判断"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 已完成的组合\n",
    "finished = {\n",
    "    ('imdb', 'gpt-3.5-turbo', 'delete'),\n",
    "    ('imdb', 'gpt-3.5-turbo', 'replace'),\n",
    "    ('imdb', 'gpt-4-turbo', 'delete'),\n",
    "    ('imdb', 'gpt-4o', 'delete'),\n",
    "}\n",
    "\n",
    "import openai\n",
    "openai_key = 'sk-' # 补全\n",
    "model_names = ['gpt-3.5-turbo', 'gpt-4-turbo','gpt-4o']\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# 模型名 & 数据集 & 攻击模式 & 扰乱概率\n",
    "model_names = ['gpt-3.5-turbo', 'gpt-4-turbo', 'gpt-4o']\n",
    "datasets = ['imdb', 'yelp', 'sst-2']\n",
    "attack_modes = ['delete', 'replace', 'insert', 'mix']\n",
    "perturb_probs = [0.75, 0.55, 0.35, 0.15]\n",
    "\n",
    "# 输出路径\n",
    "output_dir = './result_278'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for dataset_name in datasets[0:1]:\n",
    "    for attack_mode in attack_modes[0:1]:\n",
    "        for perturb_prob in perturb_probs[0:1]:\n",
    "            # 加载攻击样本\n",
    "            file_path = f'./text_processed_278/{dataset_name}_{attack_mode}_{perturb_prob}.pkl'\n",
    "            with open(file_path, 'rb') as f:\n",
    "                attacked_samples = pickle.load(f)\n",
    "            \n",
    "            for model_name in model_names:\n",
    "                if (dataset_name, model_name, attack_mode, perturb_prob) in finished:\n",
    "                    print(f\"⏭️ 已完成，跳过：{dataset_name} | {model_name} | {attack_mode} | {perturb_prob}\")\n",
    "                    continue\n",
    "                results = []\n",
    "\n",
    "                for i, sample in enumerate(tqdm(attacked_samples, desc=f'{dataset_name} | {attack_mode} | {model_name} | {perturb_prob}')):\n",
    "                    original_text = sample['original_text']\n",
    "                    attacked_text = sample['attacked_text']\n",
    "                    label = sample['label']\n",
    "\n",
    "                    messages_original = create_messages(original_text)\n",
    "                    messages_attacked = create_messages(attacked_text)\n",
    "\n",
    "                    result_original = run_llm(messages_original, api_key=openai_key, model_name=model_name)\n",
    "                    result_attacked = run_llm(messages_attacked, api_key=openai_key, model_name=model_name)\n",
    "\n",
    "                    results.append({\n",
    "                        \"id\": i,\n",
    "                        \"label\": label,\n",
    "                        \"result_original\": result_original,\n",
    "                        \"result_attacked\": result_attacked,\n",
    "                        \"original_text\": original_text,\n",
    "                        \"attacked_text\": attacked_text\n",
    "                    })\n",
    "\n",
    "                # 保存为 CSV & PKL\n",
    "                df = pd.DataFrame(results)\n",
    "                df.to_csv(f'{output_dir}/{dataset_name}_{model_name}_{attack_mode}_{perturb_prob}.csv', index=False)\n",
    "                df.to_pickle(f'{output_dir}/{dataset_name}_{model_name}_{attack_mode}_{perturb_prob}.pkl')\n",
    "                print(f\"✅ 保存成功: {dataset_name} | {model_name} | {attack_mode} | {perturb_prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "改perturb_prob和max_alterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Error ratio based global perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def text_attack_by_ratio(text, \n",
    "                         error_ratio=0.4,\n",
    "                         p_delete=0.33, \n",
    "                         p_replace=0.33, \n",
    "                         p_insert=0.34,\n",
    "                         max_per_word=2):\n",
    "    \"\"\"\n",
    "    随机选择单词进行字符级扰动，每个单词最多扰动 max_per_word 次，\n",
    "    整体最多扰动 N_max 次。\n",
    "\n",
    "    参数：\n",
    "      - text: 原文本\n",
    "      - error_ratio: 错误率（控制 N_max）\n",
    "      - p_delete, p_replace, p_insert: 扰动操作比例\n",
    "      - max_per_word: 每个单词最多可扰动几次\n",
    "\n",
    "    返回：\n",
    "      - attacked_text: 扰动后的文本\n",
    "      - stats: 扰动统计信息\n",
    "    \"\"\"\n",
    "    # 拆分单词\n",
    "    words = text.split()\n",
    "    N = sum(len(w) for w in words)\n",
    "    if N == 0:\n",
    "        return text, {\n",
    "            \"total_alterations\": 0,\n",
    "            \"delete\": 0,\n",
    "            \"replace\": 0,\n",
    "            \"insert\": 0\n",
    "        }\n",
    "\n",
    "    # 计算最大总扰动次数\n",
    "    N_max = int(round(error_ratio * N))\n",
    "    if N_max <= 0:\n",
    "        return text, {\n",
    "            \"total_alterations\": 0,\n",
    "            \"delete\": 0,\n",
    "            \"replace\": 0,\n",
    "            \"insert\": 0\n",
    "        }\n",
    "\n",
    "    # 将单词转换为字符列表，记录每个单词已修改次数\n",
    "    word_chars_list = [list(w) for w in words]\n",
    "    word_alter_counts = [0] * len(words)\n",
    "\n",
    "    # 统计信息\n",
    "    total_alterations = 0\n",
    "    count_delete, count_replace, count_insert = 0, 0, 0\n",
    "\n",
    "    attempts = 0\n",
    "    max_attempts = 10000  # 防止死循环\n",
    "\n",
    "    while total_alterations < N_max and attempts < max_attempts:\n",
    "        attempts += 1\n",
    "\n",
    "        # 随机选一个单词\n",
    "        idx = random.randrange(len(words))\n",
    "\n",
    "        # 如果该单词已达最大修改次数，则跳过\n",
    "        if word_alter_counts[idx] >= max_per_word:\n",
    "            continue\n",
    "\n",
    "        chars = word_chars_list[idx]\n",
    "\n",
    "        # 判断是否还能执行 delete/replace 操作\n",
    "        available_ops = []\n",
    "        if len(chars) > 0:\n",
    "            available_ops.extend([\"delete\", \"replace\"])\n",
    "        available_ops.append(\"insert\")\n",
    "\n",
    "        # 在可用的操作中按权重选择操作\n",
    "        weights = []\n",
    "        if \"delete\" in available_ops: weights.append(p_delete)\n",
    "        if \"replace\" in available_ops: weights.append(p_replace)\n",
    "        if \"insert\" in available_ops: weights.append(p_insert)\n",
    "\n",
    "        op_type = random.choices(\n",
    "            population=available_ops,\n",
    "            weights=weights,\n",
    "            k=1\n",
    "        )[0]\n",
    "\n",
    "        if op_type == \"delete\":\n",
    "            idx_char = random.randrange(len(chars))\n",
    "            chars.pop(idx_char)\n",
    "            count_delete += 1\n",
    "        elif op_type == \"replace\":\n",
    "            idx_char = random.randrange(len(chars))\n",
    "            chars[idx_char] = random.choice(\"abcdefghijklmnopqrstuvwxyz\")\n",
    "            count_replace += 1\n",
    "        elif op_type == \"insert\":\n",
    "            idx_char = random.randint(0, len(chars))\n",
    "            chars.insert(idx_char, random.choice(\"abcdefghijklmnopqrstuvwxyz\"))\n",
    "            count_insert += 1\n",
    "\n",
    "        word_alter_counts[idx] += 1\n",
    "        total_alterations += 1\n",
    "\n",
    "    # 拼回文本\n",
    "    attacked_words = [\"\".join(c) for c in word_chars_list]\n",
    "    attacked_text = \" \".join(attacked_words)\n",
    "\n",
    "    stats = {\n",
    "        \"total_alterations\": total_alterations,\n",
    "        \"delete\": count_delete,\n",
    "        \"replace\": count_replace,\n",
    "        \"insert\": count_insert\n",
    "    }\n",
    "    return attacked_text, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "attack_settings = [\n",
    "    # {\"name\": \"delete\", \"p_delete\": 1.0, \"p_replace\": 0.0, \"p_insert\": 0.0},\n",
    "    # {\"name\": \"replace\", \"p_delete\": 0.0, \"p_replace\": 1.0, \"p_insert\": 0.0},\n",
    "    # {\"name\": \"insert\", \"p_delete\": 0.0, \"p_replace\": 0.0, \"p_insert\": 1.0},\n",
    "    {\"name\": \"mix\", \"p_delete\": 0.33, \"p_replace\": 0.34, \"p_insert\": 0.33},\n",
    "]\n",
    "error_ratio = 0.4\n",
    "\n",
    "# 设置随机种子，保证攻击文本一致\n",
    "random.seed(42)\n",
    "\n",
    "# 加载数据集\n",
    "# datasets = [\"imdb\", \"yelp\"]\n",
    "datasets = [\"sst-2\"]\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    with open(f\"./dataset_278/{dataset_name}.pkl\", \"rb\") as f:\n",
    "        dataset = pickle.load(f)\n",
    "        # dataset = dataset[0:1]  # 可解注用于调试小样本\n",
    "\n",
    "    for attack_config in attack_settings:\n",
    "        setting_name = attack_config[\"name\"]\n",
    "        p_delete = attack_config[\"p_delete\"]\n",
    "        p_replace = attack_config[\"p_replace\"]\n",
    "        p_insert = attack_config[\"p_insert\"]\n",
    "\n",
    "        # Step 1: 预生成扰动文本，缓存下来\n",
    "        attacked_samples = []\n",
    "        for item in dataset:\n",
    "            original_text = item[\"sentence\"]\n",
    "            # original_text = item[\"text\"]  # 如果字段名是 text，请改为此行\n",
    "\n",
    "            attacked_text, stats = text_attack_by_ratio(\n",
    "                original_text,\n",
    "                error_ratio=error_ratio,\n",
    "                p_delete=p_delete,\n",
    "                p_replace=p_replace,\n",
    "                p_insert=p_insert,\n",
    "                max_per_word=2,\n",
    "            )\n",
    "            attacked_samples.append({\n",
    "                \"label\": item[\"label\"],\n",
    "                \"original_text\": original_text,\n",
    "                \"attacked_text\": attacked_text,\n",
    "                \"status\": stats\n",
    "            })\n",
    "\n",
    "        print(attacked_samples[:3])  # 只打印前3条检查格式\n",
    "        print(f\"总样本数: {len(attacked_samples)}\")\n",
    "\n",
    "        # 保存扰动样本为 pickle 文件\n",
    "        save_path = f'./ratio_278/{dataset_name}_{setting_name}_{error_ratio}.pkl'\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "\n",
    "        with open(save_path, 'wb') as f:\n",
    "            pickle.dump(attacked_samples, f)\n",
    "\n",
    "        # 保存为 CSV 文件\n",
    "        df = pd.DataFrame(attacked_samples)\n",
    "        df.to_csv(f'./ratio_278/{dataset_name}_{setting_name}_{error_ratio}.csv', index=False)\n",
    "\n",
    "        print(f\"✅ 已保存：{save_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
